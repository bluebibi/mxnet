{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Convolutional Neural Networks (LeNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP's problems for the Fashion-MNIST dataset\n",
    "  - The pixels in the image are expanded line by line to get a vector of length 784, and then used them as inputs to the fully connected layer.\n",
    "  - The adjacent pixels in the same column of an image may be far apart in this vector. \n",
    "    - The patterns they create may be difficult for the model to recognize. \n",
    "    - The vectorial representation ignores position entirely\n",
    "  - For large input images, using a fully connected layer can easily cause the model to become too large.\n",
    "- CNN\n",
    "  - The convolutional layer retains the input shape\n",
    "    - The correlation of image pixels in the directions of both height and width can be recognized effectively.\n",
    "  - The convolutional layer repeatedly calculates the same kernel and the input of different positions through the sliding window\n",
    "    - It avoids excessively large parameter sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.1 LeNet\n",
    "- LeNet\n",
    "  - invented by Yann LeCun and coworkers at AT&T Bell Labs in the early 90s.\n",
    "- LeNet is divided into two parts\n",
    "  - Convolutional layers \n",
    "  - Fully connected layers\n",
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/lenet.svg?sanitize=true)\n",
    "\n",
    "- The basic units in the convolutional block\n",
    "  - a convolutional layer \n",
    "    - used to recognize the spatial patterns in the image, such as lines and the parts of objects\n",
    "    - $5\\times 5$ kernel window\n",
    "    - sigmoid activation function\n",
    "      - [NOTE] ReLu works better, but it had not been invented in the 90s yet.\n",
    "    - The number of output channels\n",
    "      - For the first convolutional layer is 6\n",
    "    - The number of output channels \n",
    "      - For the second convolutional layer is 16\n",
    "  - a subsequent average pooling layer \n",
    "    - used to reduce the dimensionality\n",
    "      - [NOTE] max-pooling works better, but it had not been invented in the 90s yet.\n",
    "    - The window shape for the two average pooling layers of the convolutional layer block is $2\\times 2$\n",
    "    - The stride is 2\n",
    "    - the pooling layer performs downsampling\n",
    "- Height and width of the input of the second convolutional layer is smaller than that of the first convolutional layer. \n",
    "  - Therefore, increasing the number of output channels makes the parameter sizes of the two convolutional layers similar. \n",
    "  \n",
    "- The output shape of the convolutional layer block is (batch size, channel, height, width). \n",
    "- When the output of the convolutional layer block is passed into the fully connected layer block, the fully connected layer block flattens each example in the mini-batch. \n",
    "  - The input shape of the fully connected layer will become two dimensional is (batch size, channel $\\times$ height $\\times$ width)\n",
    "  - The fully connected layer block has three fully connected layers. \n",
    "    - They have 120, 84, and 10 outputs, respectively. \n",
    "    - Here, 10 is the number of output classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='sigmoid'),\n",
    "    nn.AvgPool2D(pool_size=2, strides=2),\n",
    "    nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "    nn.AvgPool2D(pool_size=2, strides=2),\n",
    "    # Dense will transform the input of the shape (batch size, channel, height, width) into\n",
    "    # the input of the shape (batch size, channel *height * width) automatically by default.\n",
    "    nn.Dense(120, activation='sigmoid'),\n",
    "    nn.Dense(84, activation='sigmoid'),\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: conv133, Parameter Shape: Weight-(6, 1, 5, 5)/Bias-(6,), Output Shape: (1, 6, 28, 28)\n",
      "Layer Name: pool83, Output Shape: (1, 6, 14, 14)\n",
      "Layer Name: conv134, Parameter Shape: Weight-(16, 6, 5, 5)/Bias-(16,), Output Shape: (1, 16, 10, 10)\n",
      "Layer Name: pool84, Output Shape: (1, 16, 5, 5)\n",
      "Layer Name: dense60, Parameter Shape: Weight-(120, 400)/Bias-(120,), Output Shape: (1, 120)\n",
      "Layer Name: dense61, Parameter Shape: Weight-(84, 120)/Bias-(84,), Output Shape: (1, 84)\n",
      "Layer Name: dense62, Parameter Shape: Weight-(10, 84)/Bias-(10,), Output Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    if layer.name.startswith(\"pool\"):\n",
    "        print(\"Layer Name: {0}, Output Shape: {1}\".format(\n",
    "            layer.name,\n",
    "            X.shape\n",
    "        ))       \n",
    "    else:\n",
    "        print(\"Layer Name: {0}, Parameter Shape: Weight-{1}/Bias-{2}, Output Shape: {3}\".format(\n",
    "            layer.name,\n",
    "            layer.weight.data().shape,\n",
    "            layer.bias.data().shape,            \n",
    "            X.shape\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/lenet-vert.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.2 Data Acquisition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu4(): # This function has been saved in the gluonbook package for future use.\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx) \n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu() \n",
    "    return ctx\n",
    "\n",
    "ctx = try_gpu4()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc = nd.array([0], ctx=ctx)\n",
    "    for X, y in data_iter:\n",
    "        # If ctx is the GPU, copy the data to the GPU.\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "        acc += gb.accuracy(net(X), y)\n",
    "    return acc.asscalar() / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function has been saved in the gluonbook package for future use.\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    print('Training on', ctx)\n",
    "\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_sum = 0.0 \n",
    "        train_acc_sum = 0.0\n",
    "        start = time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss_sum += l.mean().asscalar()\n",
    "            train_acc_sum += gb.accuracy(y_hat, y)\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('Epoch {%d}, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (\n",
    "            epoch + 1, \n",
    "            train_loss_sum / len(train_iter),\n",
    "            train_acc_sum / len(train_iter),\n",
    "            test_acc, \n",
    "            time.time() - start\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu(0)\n",
      "Epoch {1}, loss 2.3193, train acc 0.101, test acc 0.099, time 18.3 sec\n",
      "Epoch {2}, loss 2.0512, train acc 0.234, test acc 0.548, time 18.0 sec\n",
      "Epoch {3}, loss 1.0410, train acc 0.587, test acc 0.657, time 18.1 sec\n",
      "Epoch {4}, loss 0.8653, train acc 0.664, test acc 0.701, time 18.6 sec\n",
      "Epoch {5}, loss 0.7620, train acc 0.707, test acc 0.727, time 18.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 Deep Convolutional Neural Networks (AlexNet)\n",
    "- A good story about machine learning for image data\n",
    "- Deep learning vs. Machine learning\n",
    "  - Deep learning is end-to-end learning.\n",
    "  - In machine learning, feature enginnering is very important\n",
    "    - SIFT, SURF, HOG, Bags of visual words and similar feature extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.1 Learning Feature Representation\n",
    "- Deep learning researchers believed that...\n",
    "  - ***features themselves ought to be learned***.\n",
    "  - ***features ought to be hierarchically composed***.\n",
    "  - By jointly training many layers of a neural network, they might come to learn hierarchical representations of data. \n",
    "- In 2012, Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton designed a new variant of a CNN, ***AlexNet***\n",
    "  - It achieved excellent performance in the ImageNet challenge. \n",
    "  - It learned good feature extractors in the lower layers. \n",
    "  - The figure below describes lower level image descriptors.\n",
    "    - Image filters learned by the first layer of AlexNet\n",
    "  ![](https://github.com/d2l-ai/d2l-en/raw/master/img/filters.png)\n",
    "  - Higher layers might build upon these representations to represent larger structures\n",
    "    - like eyes, noses, blades of grass, and features. \n",
    "  - Yet higher layers might represent whole objects\n",
    "    - like people, airplanes, dogs, or frisbees. \n",
    "  - And ultimately, before the classification layer, the final hidden state might represent a compact representation of the image \n",
    "    - It summarizes the contents where data belonging to different categories would be linearly separable. \n",
    "  - The hierarchical representation of the input is determined by the parameters in the multilayer model, and these parameters are all obtained from learning.\n",
    "- Visual processing system of animals (and humans) works a bit like that. \n",
    "  - At its lowest level, it contains mostly edge detectors, followed by more structured features. \n",
    "- The main reasons why in the 90s and early 2000s algorithms based on convex optimization were the preferred way of solving problems.\n",
    "  - Missing Ingredient - ***Data***\n",
    "  - Missing Ingredient - ***Hardware***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.2 AlexNet\n",
    "- This network proved, for the first time, that the features obtained by learning can transcend manually-design features, breaking the previous paradigm in computer vision. \n",
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/alexnet-all.svg?sanitize=true![image.png](attachment:image.png))\n",
    "- Capacity Control and Preprocessing\n",
    "  - AlextNet controls the model complexity of the fully connected layer by ***dropout***\n",
    "    - LeNet only uses weight decay. \n",
    "  - AlexNet added a great deal of image augmentation, such as flipping, clipping, and color changes. \n",
    "    - This makes the model more robust\n",
    "    - The larger sample size effectively reduces overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "net = nn.Sequential()\n",
    "# Here, we use a larger 11 x 11 window to capture objects. \n",
    "# At the same time, we use a stride of 4 to greatly reduce the height and width of the output.\n",
    "# Here, the number of output channels is much larger than that in LeNet.\n",
    "net.add(\n",
    "    nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    \n",
    "    # Make the convolution window smaller, \n",
    "    # set padding to 2 for consistent height and width across the input and output, \n",
    "    # and increase the number of output channels\n",
    "    nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    \n",
    "    # Use three successive convolutional layers and a smaller convolution window. \n",
    "    # Except for the final convolutional layer, the number of output channels is further increased.\n",
    "    # Pooling layers are not used to reduce the height and width of input after the first two convolutional layers.\n",
    "    nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "    nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "    nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "\n",
    "    # Here, the number of outputs of the fully connected layer is several times larger than that in LeNet. \n",
    "    # Use the dropout layer to mitigate overfitting.\n",
    "    nn.Dense(4096, activation=\"relu\"),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Dense(4096, activation=\"relu\"),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    # Output layer. Since we are using Fashion-MNIST, the number of classes is 10, instead of 1000 as in the paper.\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: conv135, Parameter Shape: Weight-(96, 1, 11, 11)/Bias-(96,), Output Shape: (1, 96, 54, 54)\n",
      "Layer Name: pool85, Output Shape: (1, 96, 26, 26)\n",
      "Layer Name: conv136, Parameter Shape: Weight-(256, 96, 5, 5)/Bias-(256,), Output Shape: (1, 256, 26, 26)\n",
      "Layer Name: pool86, Output Shape: (1, 256, 12, 12)\n",
      "Layer Name: conv137, Parameter Shape: Weight-(384, 256, 3, 3)/Bias-(384,), Output Shape: (1, 384, 12, 12)\n",
      "Layer Name: conv138, Parameter Shape: Weight-(384, 384, 3, 3)/Bias-(384,), Output Shape: (1, 384, 12, 12)\n",
      "Layer Name: conv139, Parameter Shape: Weight-(256, 384, 3, 3)/Bias-(256,), Output Shape: (1, 256, 12, 12)\n",
      "Layer Name: pool87, Output Shape: (1, 256, 5, 5)\n",
      "Layer Name: dense63, Parameter Shape: Weight-(4096, 6400)/Bias-(4096,), Output Shape: (1, 4096)\n",
      "Layer Name: dropout38, Output Shape: (1, 4096)\n",
      "Layer Name: dense64, Parameter Shape: Weight-(4096, 4096)/Bias-(4096,), Output Shape: (1, 4096)\n",
      "Layer Name: dropout39, Output Shape: (1, 4096)\n",
      "Layer Name: dense65, Parameter Shape: Weight-(10, 4096)/Bias-(10,), Output Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    if layer.name.startswith(\"pool\") or layer.name.startswith(\"dropout\"):\n",
    "        print(\"Layer Name: {0}, Output Shape: {1}\".format(\n",
    "            layer.name,\n",
    "            X.shape\n",
    "        ))       \n",
    "    else:\n",
    "        print(\"Layer Name: {0}, Parameter Shape: Weight-{1}/Bias-{2}, Output Shape: {3}\".format(\n",
    "            layer.name,\n",
    "            layer.weight.data().shape,\n",
    "            layer.bias.data().shape,            \n",
    "            X.shape\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can upsample a fashion-MNIST sample image to 244 × 244 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function has been saved in the gluonbook package for future use.\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join('~', '.mxnet', 'datasets', 'fashion-mnist')):\n",
    "    root = os.path.expanduser(root)  # Expand the user path '~'.\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "    \n",
    "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
    "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
    "    \n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "    \n",
    "    train_iter = gdata.DataLoader(\n",
    "        mnist_train.transform_first(transformer), batch_size, shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_iter = gdata.DataLoader(\n",
    "        mnist_test.transform_first(transformer), batch_size, shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 5\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.initialize(ctx=ctx, init=init.Xavier(), force_reinit=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Small AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "net = nn.Sequential()\n",
    "# Here, we use a larger 11 x 11 window to capture objects. \n",
    "# At the same time, we use a stride of 4 to greatly reduce the height and width of the output.\n",
    "# Here, the number of output channels is much larger than that in LeNet.\n",
    "net.add(\n",
    "    nn.Conv2D(24, kernel_size=2, strides=1, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    \n",
    "    # Make the convolution window smaller, \n",
    "    # set padding to 2 for consistent height and width across the input and output, \n",
    "    # and increase the number of output channels\n",
    "    nn.Conv2D(64, kernel_size=2, padding=1, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    \n",
    "    # Use three successive convolutional layers and a smaller convolution window. \n",
    "    # Except for the final convolutional layer, the number of output channels is further increased.\n",
    "    # Pooling layers are not used to reduce the height and width of input after the first two convolutional layers.\n",
    "    nn.Conv2D(96, kernel_size=2, padding=1, activation='relu'),\n",
    "    nn.Conv2D(96, kernel_size=2, padding=1, activation='relu'),\n",
    "    nn.Conv2D(64, kernel_size=2, padding=1, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "\n",
    "    # Here, the number of outputs of the fully connected layer is several times larger than that in LeNet. \n",
    "    # Use the dropout layer to mitigate overfitting.\n",
    "    nn.Dense(1024, activation=\"relu\"),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Dense(1024, activation=\"relu\"),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    # Output layer. Since we are using Fashion-MNIST, the number of classes is 10, instead of 1000 as in the paper.\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: conv140, Parameter Shape: Weight-(24, 1, 2, 2)/Bias-(24,), Output Shape: (1, 24, 27, 27)\n",
      "Layer Name: pool88, Output Shape: (1, 24, 13, 13)\n",
      "Layer Name: conv141, Parameter Shape: Weight-(64, 24, 2, 2)/Bias-(64,), Output Shape: (1, 64, 14, 14)\n",
      "Layer Name: pool89, Output Shape: (1, 64, 6, 6)\n",
      "Layer Name: conv142, Parameter Shape: Weight-(96, 64, 2, 2)/Bias-(96,), Output Shape: (1, 96, 7, 7)\n",
      "Layer Name: conv143, Parameter Shape: Weight-(96, 96, 2, 2)/Bias-(96,), Output Shape: (1, 96, 8, 8)\n",
      "Layer Name: conv144, Parameter Shape: Weight-(64, 96, 2, 2)/Bias-(64,), Output Shape: (1, 64, 9, 9)\n",
      "Layer Name: pool90, Output Shape: (1, 64, 4, 4)\n",
      "Layer Name: dense66, Parameter Shape: Weight-(1024, 1024)/Bias-(1024,), Output Shape: (1, 1024)\n",
      "Layer Name: dropout40, Output Shape: (1, 1024)\n",
      "Layer Name: dense67, Parameter Shape: Weight-(1024, 1024)/Bias-(1024,), Output Shape: (1, 1024)\n",
      "Layer Name: dropout41, Output Shape: (1, 1024)\n",
      "Layer Name: dense68, Parameter Shape: Weight-(10, 1024)/Bias-(10,), Output Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    if layer.name.startswith(\"pool\") or layer.name.startswith(\"dropout\"):\n",
    "        print(\"Layer Name: {0}, Output Shape: {1}\".format(\n",
    "            layer.name,\n",
    "            X.shape\n",
    "        ))       \n",
    "    else:\n",
    "        print(\"Layer Name: {0}, Parameter Shape: Weight-{1}/Bias-{2}, Output Shape: {3}\".format(\n",
    "            layer.name,\n",
    "            layer.weight.data().shape,\n",
    "            layer.bias.data().shape,            \n",
    "            X.shape\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 1.6177, train acc 0.379, test acc 0.640, time 96.9 sec\n",
      "epoch 2, loss 0.8953, train acc 0.654, test acc 0.740, time 95.2 sec\n",
      "epoch 3, loss 0.7166, train acc 0.725, test acc 0.770, time 95.0 sec\n",
      "epoch 4, loss 0.6218, train acc 0.762, test acc 0.794, time 95.2 sec\n",
      "epoch 5, loss 0.5587, train acc 0.785, test acc 0.824, time 94.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "num_epochs = 5\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.8 Networks Using Blocks (VGG)\n",
    "- Progress in this field mirrors that in chip design where engineers went from placing transistors (neurons) to logical elements (layers) to logic blocks (the topic of the current section). \n",
    "- The idea of using blocks was first proposed by the Visual Geometry Group (VGG) at Oxford University. \n",
    "- When using a modern deep learning framework, repeated structures can be expressed as code with for loops and subroutines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8.1 VGG Blocks\n",
    "- The basic building block of a ConvNet is the combination of a convolutional layer (with padding to keep the resolution unchanged), followed by a nonlinearity such as a ReLu. \n",
    "- A VGG block is given by a sequence of such layers, followed by maximum pooling. \n",
    "- In 2014, Simonyan and Ziserman used...\n",
    "  - convolution windows of size 3\n",
    "  - maximum pooling with window width 2\n",
    "  - 2 stride \n",
    "  - effectively halving the resolution after each block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=2, padding=1, activation='relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8.2 VGG Network\n",
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/vgg.svg?sanitize=true)\n",
    "- Several `vgg_block` modules are connected in series in the convolutional layer module, the hyper-parameter of which is defined by the variable `conv_arch`. \n",
    "  - This variable specifies the numbers of convolutional layers and output channels in each VGG block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 256), (2, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this network uses 8 convolutional layers and 3 fully connected layers, it is often called `VGG-11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "    # The convolutional layer part.\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs, num_channels))\n",
    "        \n",
    "    # The fully connected layer part.\n",
    "    net.add(\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "    return net\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By halving the height and width while doubling the number of channels, VGG allows most convolutional layers to have the same model activation size and computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 6\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blk Name: sequential91, Output Shape: (1, 10, 14, 14)\n",
      "blk Name: sequential92, Output Shape: (1, 21, 7, 7)\n",
      "blk Name: sequential93, Output Shape: (1, 42, 4, 4)\n",
      "blk Name: sequential94, Output Shape: (1, 42, 3, 3)\n",
      "blk Name: sequential95, Output Shape: (1, 42, 2, 2)\n",
      "blk Name: dense72, Output Shape: (1, 1024)\n",
      "blk Name: dropout44, Output Shape: (1, 1024)\n",
      "blk Name: dense73, Output Shape: (1, 1024)\n",
      "blk Name: dropout45, Output Shape: (1, 1024)\n",
      "blk Name: dense74, Output Shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(\"blk Name: {0}, Output Shape: {1}\".format(\n",
    "        blk.name,\n",
    "        X.shape\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.2859, train acc 0.223, test acc 0.399, time 51.5 sec\n",
      "epoch 2, loss 1.3229, train acc 0.482, test acc 0.638, time 51.3 sec\n",
      "epoch 3, loss 0.8787, train acc 0.649, test acc 0.707, time 51.5 sec\n",
      "epoch 4, loss 0.7683, train acc 0.708, test acc 0.758, time 51.6 sec\n",
      "epoch 5, loss 0.6797, train acc 0.743, test acc 0.785, time 51.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.initialize(ctx=ctx, init=init.Xavier(), force_reinit=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGG-11 constructs a network using reusable convolutional blocks. \n",
    "- Different VGG models can be defined by the differences in the number of convolutional layers and output channels in each block.\n",
    "- The use of blocks leads to very compact representations of the network definition. \n",
    "- It allows for efficient design of complex networks.\n",
    "- Simonyan and Ziserman experimented with various architetures. \n",
    "- In particular, they found that several layers of deep and narrow convolutions (i.e. 3 × 3) were more effective than fewer layers of wider convolutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
