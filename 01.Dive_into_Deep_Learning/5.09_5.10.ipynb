{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.9 Network in Network (NiN)\n",
    "- LeNet, AlexNet, and VGG all share a common design pattern\n",
    "  - extract the spatial features through a sequence of convolutions \n",
    "  - pooling layers \n",
    "  - post-process the representations via fully connected layers\n",
    "- A careless use of a dense layer would destroy the spatial structure of the data entirely, since fully connected layers mangle all inputs.\n",
    "- NiN \n",
    "  - Lin, Chen and Yan, \"Network In Network,\" 2013 - https://arxiv.org/pdf/1312.4400.pdf\n",
    "  - Use an MLP on the channels for each pixel separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9.1 NiN Blocks\n",
    "- The inputs and outputs of convolutional layers are usually four-dimensional arrays \n",
    "  - (example, channel, height, width)\n",
    "- The inputs and outputs of fully connected layers are usually two-dimensional arrays\n",
    "  - (example, feature)\n",
    "- Once we process data by a fully connected layer, it's virtually impossible to recover the spatial structure of the representation. \n",
    "- We could apply a fully connected layer at a pixel level\n",
    "  - Recall the $1\\times 1$ convolutional layer. \n",
    "  - It is considered as a fully connected layer processing channel activations on a per pixel level. \n",
    "  - Another way to view this is to think of...\n",
    "    - each element in the spatial dimension (height and width) as equivalent to an example, \n",
    "    - the channel as equivalent to a feature. \n",
    "- NiN Block\n",
    "  - it use the $1\\times 1$ convolutional layer instead of a fully connected layer. \n",
    "  - The spatial information can then be naturally passed to the subsequent layers.\n",
    "  - It consists of a convolutional layer and multiple $1\\times 1$ convolutional layer. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.\n",
    "  ![](https://github.com/d2l-ai/d2l-en/raw/master/img/nin-compare.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(\n",
    "        nn.Conv2D(num_channels, kernel_size, strides, padding, activation='relu'),\n",
    "        nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "        nn.Conv2D(num_channels, kernel_size=1, activation='relu')\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9.2 NiN Model\n",
    "- NiN uses convolutional layers with convolution window shapes of 11 × 11, 5 × 5, and 3 × 3, and the corresponding numbers of output channels (96, 256, and 384) are the same as in AlexNet. \n",
    "- Each NiN block is followed by a maximum pooling layer with a stride of 2 and a window shape of 3 × 3.\n",
    "- The last NiN block has a number of output channels equal to the number of label classes, and then uses a global average pooling layer to average all elements in each channel for direct use in classification. \n",
    "- In global average pooling layer, the window shape is equal to the average pooling layer of the input spatial dimension shape. \n",
    "- It can significantly reduce the size of model parameters, thus mitigating overfitting. \n",
    "- In other words, all operations are convolutions. \n",
    "- However, this design sometimes results in an increase in model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nin_block(24, kernel_size=3, strides=1, padding=0),\n",
    "    nn.MaxPool2D(pool_size=2, strides=1),\n",
    "    nin_block(64, kernel_size=3, strides=2, padding=2),\n",
    "    nn.MaxPool2D(pool_size=2, strides=1),\n",
    "    nin_block(96, kernel_size=4, strides=2, padding=1),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    # There are 10 label classes.\n",
    "    nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "    \n",
    "    # The global average pooling layer automatically sets the window shape to the height and width of the input.\n",
    "    nn.GlobalAvgPool2D(),\n",
    "    \n",
    "    # Transform the four-dimensional output into two-dimensional output with a shape of (batch size, 10).\n",
    "    nn.Flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer.name sequential71 - Output Shape - (1, 24, 26, 26)\n",
      "Layer.name pool56       - Output Shape - (1, 24, 25, 25)\n",
      "Layer.name sequential72 - Output Shape - (1, 64, 14, 14)\n",
      "Layer.name pool57       - Output Shape - (1, 64, 13, 13)\n",
      "Layer.name sequential73 - Output Shape - (1, 96, 6, 6)\n",
      "Layer.name pool58       - Output Shape - (1, 96, 2, 2)\n",
      "Layer.name dropout14    - Output Shape - (1, 96, 2, 2)\n",
      "Layer.name sequential74 - Output Shape - (1, 10, 2, 2)\n",
      "Layer.name pool59       - Output Shape - (1, 10, 1, 1)\n",
      "Layer.name flatten14    - Output Shape - (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize(force_reinit=True)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(\"Layer.name {0:12s} - Output Shape - {1}\".format(layer.name, X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9.3 Data Acquisition and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NiN removes the fully connected layers and replaces them with global average pooling (i.e. summing over all locations) after reducing the number of channels to the desired number of outputs\n",
    "- Removing the dense layers reduces overfitting. NiN has dramatically fewer parameters.\n",
    "- The NiN design influenced many subsequent convolutional neural networks designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.3018, train acc 0.126, test acc 0.240, time 158.0 sec\n",
      "epoch 2, loss 2.2965, train acc 0.199, test acc 0.236, time 155.7 sec\n",
      "epoch 3, loss 2.2469, train acc 0.214, test acc 0.235, time 159.2 sec\n",
      "epoch 4, loss 1.9712, train acc 0.315, test acc 0.460, time 154.5 sec\n",
      "epoch 5, loss 1.6260, train acc 0.446, test acc 0.498, time 158.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.10 Networks with Parallel Concatenations (GoogLeNet)\n",
    "- Szegedy et al., \"Going Deeper with Convolutions,\" 2014 - https://arxiv.org/abs/1409.4842\n",
    "  - Pragmatic answer to the question as to which size of convolution is ideal for processing.\n",
    "    - 1 × 1 or 3 × 3, 5 × 5 or even larger. \n",
    "  - It isn’t always clear which one is the best. \n",
    "  - As it turns out, the answer is that a combination of all the above works best.\n",
    "- Over the next few years, researchers made several improvements to GoogLeNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10.1 Inception Blocks\n",
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/inception.svg?sanitize=true)\n",
    "- There are four parallel paths in the Inception block. \n",
    "- The first three paths \n",
    "  - use convolutional layers with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$ \n",
    "  - to extract information from different spatial sizes. \n",
    "- The middle two paths \n",
    "  - perform a $1\\times 1$ convolution on the input to reduce the number of input channels\n",
    "  - so as to reduce the model's complexity. \n",
    "- The fourth path \n",
    "  - uses the $3\\times 3$ maximum pooling layer\n",
    "  - followed by the $1\\times 1$ convolutional layer to change the number of channels. \n",
    "- The four paths all use appropriate padding to give the input and output the same height and width. \n",
    "- Finally, we concatenate the output of each path on the channel dimension and input it to the next layer. \n",
    "- The customizable parameters of the Inception block\n",
    "  - Number of output channels per layer, which can be used to control the model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    # c1 - c4 are the number of output channels for each layer in the path.\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        \n",
    "        # Path 1 is a single 1 x 1 convolutional layer.\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3 convolutional layer.\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')\n",
    "\n",
    "        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5 convolutional layer.\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')\n",
    "        \n",
    "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1 convolutional layer.\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        # Concatenate the outputs on the channel dimension.\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To understand why this works as well as it does, consider the combination of the filters.\n",
    "  - Details can be recognized efficiently by different filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10.2 GoogLeNet Model\n",
    "- GoogLeNet uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates.\n",
    "- Maximum pooling between inception blocks reduced the dimensionality. \n",
    "- The first part is identical to AlexNet and LeNet\n",
    "- The stack of blocks is inherited from VGG\n",
    "- The global average pooling which can avoid a stack of fully connected layers at the end. \n",
    "![](https://github.com/d2l-ai/d2l-en/raw/master/img/inception-full.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential()\n",
    "b1.add(\n",
    "    nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential()\n",
    "b2.add(\n",
    "    nn.Conv2D(64, kernel_size=1),\n",
    "    nn.Conv2D(192, kernel_size=3, padding=1),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential()\n",
    "b3.add(\n",
    "    Inception(64, (96, 128), (16, 32), 32),\n",
    "    Inception(128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential()\n",
    "b4.add(\n",
    "    Inception(192, (96, 208), (16, 48), 64),\n",
    "    Inception(160, (112, 224), (24, 64), 64),\n",
    "    Inception(128, (128, 256), (24, 64), 64),\n",
    "    Inception(112, (144, 288), (32, 64), 64),\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential()\n",
    "b5.add(\n",
    "    Inception(256, (160, 320), (32, 128), 128),\n",
    "    Inception(384, (192, 384), (48, 128), 128),\n",
    "    nn.GlobalAvgPool2D()\n",
    ")\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential77 output shape:\t (1, 64, 24, 24)\n",
      "sequential78 output shape:\t (1, 192, 12, 12)\n",
      "sequential79 output shape:\t (1, 480, 6, 6)\n",
      "sequential80 output shape:\t (1, 832, 3, 3)\n",
      "sequential81 output shape:\t (1, 1024, 1, 1)\n",
      "dense0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10.3 Data Acquisition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.1636, train acc 0.207, test acc 0.280, time 526.9 sec\n",
      "epoch 2, loss 1.1490, train acc 0.519, test acc 0.687, time 536.9 sec\n",
      "epoch 3, loss 0.8065, train acc 0.693, test acc 0.771, time 485.1 sec\n",
      "epoch 4, loss 0.6022, train acc 0.768, test acc 0.811, time 474.9 sec\n",
      "epoch 5, loss 0.5084, train acc 0.805, test acc 0.831, time 477.2 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "gb.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Inception block is equivalent to a subnetwork with four paths. \n",
    "- It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers. \n",
    "- $1 \\times 1$ convolutions reduce channel dimensionality on a per-pixel level. \n",
    "- Max-pooling reduces the resolution.\n",
    "- GoogLeNet connects multiple well-designed Inception blocks with other layers in series. \n",
    "- The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet data set.\n",
    "- GoogLeNet, as well as its succeeding versions, was one of the most efficient models on ImageNet, providing similar test accuracy with lower computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
